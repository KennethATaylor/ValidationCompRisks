---
title: "External validation of the performance of competing risks prediction models: a guide through modern methods - developing a Fine and Gray model"

always_allow_html: true
output: 
    github_document:
    toc: true
    keep_text: true
    pandoc_args: --webtex
---

- [Goals](#goals)
  * [Install/load packages and import data](#install-load-packages-and-import-data)
  * [Descriptive statistics](#descriptive-statistics)
- [Goal 1: develop a risk prediction model with a time-to-event outcome](#goal-1--develop-a-risk-prediction-model-with-a-time-to-event-outcome)
  * [1.1 Preliminary investigation: survival and censoring curves in the development and validation data](#11-preliminary-investigation--survival-and-censoring-curves-in-the-development-and-validation-data)
  * [1.2 Secondary investigation: check non-linearity of continuous predictors](#12-secondary-investigation--check-non-linearity-of-continuous-predictors)
  * [1.3 Model development: first check - the proportional hazard (PH) assumption](#13-model-development--first-check---the-proportional-hazard--ph--assumption)
  * [1.4 Model development: fit the risk prediction models](#14-model-development--fit-the-risk-prediction-models)
- [Goal 2: Assessing performance in survival prediction models](#goal-2--assessing-performance-in-survival-prediction-models)
  * [2.1 Overall performance measures](#21-overall-performance-measures)
  * [2.2 Discrimination measures](#22-discrimination-measures)
  * [2.3 Calibration](#23-calibration)
  * [2.3.1 Observed/Expected ratio](#231-observed-expected-ratio)
  * [2.3.2 Calibration plot using restricted cubic splines](#232-calibration-plot-using-restricted-cubic-splines)
- [Goal 3: Clinical utility](#goal-3--clinical-utility)
- [References](#references)



## Goal
The goals are:   
1. To develop a competing risk prediction model;  
2. To assess the performance of a competing risk prediction model;  
3. To assess the potential clinical utility of a competing risk prediction model;

### Install/load packages and import data
We following libraries are needed to achieve the following goals, if you have not them installed, please use
install.packages('') (e.g. install.packages('survival')) or use the user-friendly approach if you are using RStudio.

```{r, wdlib, message=FALSE,warning=FALSE}

# Libraries needed
library(rio)
library(survival)
library(rms)
library(mstate)
library(sqldf)
library(pec)
library(riskRegression)
library(survAUC)
library(survivalROC)
library(timeROC)
library(plotrix)
library(splines)
library(knitr)
library(table1)
library(kableExtra)
library(boot)
library(tidyverse)
library(rsample)
library(gridExtra)
library(webshot)
# webshot::install_phantomjs()

rdata<-readRDS('C:\\Users\\danie\\Documents\\GitHub\\ValidationCompRisks\\Data\\rdata.rds')

vdata<-readRDS('C:\\Users\\danie\\Documents\\GitHub\\ValidationCompRisks\\Data\\vdata.rds')

```

We loaded the development (rdata) and the validation data (vdata) 
 

### Descriptive statistics
```{r, import,fig.align='center'}
rsel<-rdata[,c('id', 'age', 'size', 'ncat', 'hr_status')]
vsel<-vdata[,c('id', 'age', 'size', 'ncat', 'hr_status')]
rsel$dt<-1
vsel$dt<-2
cdata<-rbind(rsel,vsel)
cdata$dt<-factor(cdata$dt,levels=c(1,2),
                 labels=c('Development data','Validation data'))

label(cdata$age)<-'Age'
label(cdata$size)<-'Size'
label(cdata$ncat)<-'Nodal status'
label(cdata$hr_status)<-'Hormon receptor status'

# Units
units(cdata$age)<-'years'
units(cdata$size)<-'cm'
options(prType='html')
tab1<-table1(~ age + size + ncat + hr_status| dt, data=cdata, overall=FALSE, topclass="Rtable1-zebra")
# print(tab1)
rm(cdata,vsel,rsel)
```

<img src="01_predsurv_simplified_files/figure-gfm/tab1.png" style="display: block; margin: auto;" />


## Goal: develop a risk prediction model with a time-to-event outcome
  

### Primary investigation: cumulative incidence curves
First, we draw the cumulative incidence curves of breast cancer recurrence.

```{r, cuminc, fig.align='center'}
# Expand datasets 
# Create indicator variables for the outcome
rdata$status_num<-as.numeric(rdata$status)-1
rdata$status1[rdata$status_num==1]<-1
rdata$status1[rdata$status_num!=1]<-0
rdata$status2[rdata$status_num==2]<-2
rdata$status2[rdata$status_num!=2]<-0

# Create indicator variables for the outcome
vdata$status_num<-as.numeric(vdata$status)-1
vdata$status1[vdata$status_num==1]<-1
vdata$status1[vdata$status_num!=1]<-0
vdata$status2[vdata$status_num==2]<-2
vdata$status2[vdata$status_num!=2]<-0

# Median follow-up
# Reverse KM method censoring also competing events (so median follow-up at first event?)

# survfit(coxph(Surv(time,status_num==0)~1,data=rdata)) # 5 years

# Expand data
rdata.w<-crprep(Tstop='time',
                status='status_num',
                trans=1,
                id='id',
                keep=c('age','size','ncat','hr_status'),
                data=rdata)

vdata.w<-crprep(Tstop='time',
                status='status_num',
                trans=1,
                id='id',
                keep=c('age','size','ncat','hr_status'),
                data=vdata)

# Development set
mfit3 <- survfit(
  Surv(Tstart,Tstop,status==1)~1, 
  data=rdata.w,weights=weight.cens)

mfit4 <- survfit(
  Surv(Tstart,Tstop,status==1)~1, 
  data=vdata.w,weights=weight.cens) 

par(xaxs='i',yaxs='i',las=1)
oldpar <- par(mfrow=c(1,2), mar=c(5,5,1,1))
#par(mar=c(5.1,5.8,4.1,2.1))
#par(mgp=c(4.25,1,0))
plot(mfit3,col=1,lwd=2,
     xlab='Years since BC diagnosis',
     ylab='Cumulative incidence',bty='n',
     ylim=c(0,0.25),xlim=c(0,5),fun='event',conf.int = TRUE)
title('Development data')

plot(mfit4,col=1,lwd=2,
     xlab='Years since BC diagnosis',
     ylab='Cumulative incidence',bty='n',
     ylim=c(0,0.25),xlim=c(0,5),fun='event',conf.int = TRUE)
title('Validation data')
par(oldpar)

# Cumulative incidences
# Cumulative incidences
smfit3<-summary(mfit3,times=c(1,2,3,4,5))
smfit4<-summary(mfit4,times=c(1,2,3,4,5))

res_ci<-cbind(1-smfit3$surv,
              1-smfit3$upper,
              1-smfit3$lower,
              1-smfit4$surv,
              1-smfit4$upper,
              1-smfit4$lower)

res_ci<-round(res_ci,2)

rownames(res_ci)<-c('1-year','2-year',
                    '3-year', '4-year',
                    '5-year')
colnames(res_ci)<-rep(c('Estimate','Lower .95',
                        'Upper .95'),2)

kable(res_ci,
      row.names = TRUE) %>% 
  kable_styling('striped', position ='center') %>%   add_header_above(c(' '=1, 'Development data'=3, 'Validation data'=3))


```
The median follow-up of the development and validation data was 5 years.
The 5-year cumulative incidence of breast cancer recurrence was 14% (95% CI: 11-16%), and 10% (95%CI" 8-12%)


### Secondary investigation: check non-linearity of continuous predictors
The potential non-linear relation between continuous predictors (i.e. age and size) and the outcome should be investigated before developing a competing risk prediction model. Non-linearity of continuous predictors can be checked using three-knot restricted cubic splines using rms::rcs() function. More details are given in Frank Harrell's book 'Regression Model Strategies' on page 27 (second edition).
We developed the competing risks prediction models using the Fine and Gray model.

```{r,ff, warning=FALSE, fig.align='center'}
# FG model
dd<-datadist(rdata)
options(datadist='dd')
fit_fg_rcs<-cph(Surv(Tstart,Tstop,status==1)~rcs(age,3)+rcs(size,3)+
                  ncat+hr_status,
                weights=weight.cens,
                x=T,y=T,surv=T,data=rdata.w)
P_fg_age_rcs<-Predict(fit_fg_rcs,'age')
P_fg_size_rcs<-Predict(fit_fg_rcs,'size')
# print(fit_fg_rcs)
# print(summary(fit_fg_rcs))
# print(anova(fit_fg_rcs))

oldpar <- par(mfrow=c(1,2), mar=c(5,5,1,1))
par(xaxs='i',yaxs='i',las=1)

# FG - age
plot(P_fg_age_rcs$age,P_fg_age_rcs$yhat,
     type='l',lwd=2,col='blue',bty='n',
     xlab='Age at BC diagnosis',ylab='log Relative Hazard',ylim=c(-2,2),xlim=c(65,95))
polygon(c(P_fg_age_rcs$age,rev(P_fg_age_rcs$age)),
        c(P_fg_age_rcs$lower,rev(P_fg_age_rcs$upper)),col = "grey75", 
        border = FALSE)
par(new=TRUE)
plot(P_fg_age_rcs$age,P_fg_age_rcs$yhat,
     type='l',lwd=2,col='blue',bty='n',
     xlab='Age at BC diagnosis',ylab='log Relative Hazard',
     ylim=c(-2,2),xlim=c(65,95))

# FG - size
par(xaxs='i',yaxs='i',las=1)
plot(P_fg_size_rcs$size,P_fg_size_rcs$yhat,
     type='l',lwd=2,col='blue',bty='n',
     xlab='Size of BC',
     ylab='log Relative Hazard',ylim=c(-2,2),
     xlim=c(0,7))
polygon(c(P_fg_size_rcs$size,rev(P_fg_size_rcs$size)),
        c(P_fg_size_rcs$lower,rev(P_fg_size_rcs$upper)),col = "grey75", 
        border = FALSE)
par(new=TRUE)
plot(P_fg_size_rcs$size,P_fg_size_rcs$yhat,
     type='l',lwd=2,col='blue',bty='n',
     xlab='Size of BC',ylab='log Relative Hazard',
     ylim=c(-2,2),xlim=c(0,7))
par(oldpar)
options(datadist=NULL)
# NOTE: see oldpar

# Fit the Fine and Gray model assuming linear relation of 
# the continuous predictors
dd<-datadist(rdata)
options(datadist='dd')
fit_fg<-cph(Surv(Tstart,Tstop,status==1)~age+size+
                  ncat+hr_status,
                weights=weight.cens,
                x=T,y=T,surv=T,data=rdata.w)
options(datadist=NULL)

res_AIC<-matrix(c(AIC(fit_fg),AIC(fit_fg_rcs)),
                  byrow=T,ncol=2,nrow=1,
                  dimnames = list(c('Fine and Gray models'),
                                  c('AIC','AIC using splines')))
kable(res_AIC,
      row.names = TRUE) %>% 
  kable_styling('striped', position ='center')
```

The AIC and the graphical check suggested a potential linear relation between the continuous predictors (age and size) and the event of interest (breast cancer recurrence).


### Model development: first check - the proportional subdistribution hazard assumption
We now examine the fits in a more careful way by checking the proportionality of the subdistribution hazards of the Fine and Gray regression model.

```{r,sph, message=FALSE, warning=FALSE,fig.align='center'}
zp_fg <- cox.zph(fit_fg, transform='identity')

par(las=1,xaxs='i',yaxs='i')
# c(bottom, left, top, right)
oldpar <- par(mfrow=c(2,2), mar=c(5,6.1,3.1,1))
sub_title<-c("Age","Size","Lymph node status","HR status")
for (i in 1:4) {
  plot(zp_fg[i], resid=F,bty='n',xlim=c(0,5))
  abline(0,0, lty=3)
  title(sub_title[i])
} 
mtext("Fine and Gray", side = 3, line = -1, outer = TRUE, font=2)
par(oldpar)

kable(round(zp_fg$table,3)) %>% 
  kable_styling('striped',position = 'center')

```

The statistical tests showed a potential violation of the proportional subdistribution hazard of size of breast cancer. Size seems associated with the risk of breast cancer recurrence in the first years. Since the follow-up is short we assume, the subdistribution proportional hazard was not violated.

### 1.4 Model development: fit the risk prediction models
We develop and show the results of the Fine and Gray subdistribution hazard regression model.

```{r, summary, fig.align='center',warning=FALSE}

dd<-datadist(edata1)
options(datadist='dd')
options(prType="html")
print(fit_fg)

# We also fit the FG model using riskRegression::FGR()
# because it is useful to evaluate some of the prediction performance
# measures later
fit_fgr<-FGR(Hist(time,status_num)~age+size+
               ncat+hr_status,cause=1,data=rdata)

```
The coefficients of the models indicated that higher size, higher number of positive lymph nodes and higher grade is more associate with poorer prognosis. The association of the progesterone biomarker and the outcome is non-linear as investigated previously.


## Goal 2: Assessing performance in survival prediction models
The performance of a risk prediction models may be evaluated through:    
  + discrimination:  the ability of the model to identify patients with and without the outcome and it requires the coefficients (or the log of the subdistribution hazard ratios) of the developed risk prediction model to be evaluated.   

+ calibration: the agreement between observed and predicted probabilities.

+ overall performance measures: as a combination of discrimination and calibration and/or as a measure of the explained variation;  


### 2.1 Overall performance measures
We calculate the Brier Score and the Index of Prediction Accuracy (IPA, the scaled Brier) as a overall performance measure.  

Some confidence intervals are calculates using the bootstrap percentile method. So we use bootstrapping the development and validation data.

```{r, bootstrap,warning=FALSE}
# Bootstrapping data
set.seed(20201214)
rboot<-bootstraps(rdata,times=10)
vboot<-bootstraps(vdata,times=10)
# NOTE: B=10 to speed up the procedure for now
```

We calculate the overall performance measures: Brier score, IPA and the corresponding confidence intervals.

```{r, overall, warning=FALSE}
# Overall performance measures #

# Two measures
# - Brier score
# - IPA

# Development set - apparent validation
score_rdata1<-Score(list("FGR development"=fit_fgr),
                    formula=Hist(time,status_num)~1,
                    data=rdata,conf.int=TRUE,times=4.99,
                    cens.model = 'km', metrics='brier',
                    summary='ipa', cause=1) 

# Validation set - external validation
score_vdata1<-
  Score(list("FGR validation"=fit_fgr),
        formula=Hist(time,status_num)~1,
        data=vdata,conf.int=TRUE,times=4.99,
        cens.model = 'km', metrics='brier',
        summary='ipa') 

# Development set - internal validation (bootstrap)
# mstate::crprep() for every bootstrap sample
crprep_boot<-function(split) {
  crprep(Tstop='time', status='status_num',
         trans=1, data=analysis(split),
         keep=c('status_num','age','size',
                'ncat','hr_status'))
}

# riskRegression::Score() to calculate Brier and IPA for every bootstrap sample
score_boot_1<-function(split) {
  Score(list("FG"=fit_fgr),
        formula=Hist(time,status_num)~1,
        data=analysis(split),conf.int=FALSE,times=4.99,
        cens.model = 'km', metrics='brier',cause=1,
        summary='ipa')$Brier[[1]]$IPA[2] 
}


# Development data
rboot <- rboot %>% mutate(
  cr.prep=map(splits,crprep_boot),
  IPA1=map_dbl(splits,score_boot_1))


# Validation data
vboot <- vboot %>% mutate(
  cr.prep=map(splits,crprep_boot),
  IPA1=map_dbl(splits,score_boot_1),
)

# Table overall measures
alpha<-.05
k<-2 # number of digits
res_ov_fgr<-matrix(unlist(c(score_rdata1$Brier$score$Brier[2], # Brier apparent validation 
                        score_rdata1$Brier$score[2,6],
                        score_rdata1$Brier$score[2,7],
                        
                        score_vdata1$Brier$score$Brier[2], # Brier external validation
                        score_vdata1$Brier$score[2,6],
                        score_vdata1$Brier$score[2,7],
                        
                        score_rdata1$Brier$score$IPA[2],  # IPA apparent validation
                        quantile(rboot$IPA1,probs=alpha/2),
                        quantile(rboot$IPA1,probs=1-alpha/2),
                        
                        score_vdata1$Brier$score$IPA[2],  # IPA external validation 
                        quantile(vboot$IPA1,probs=alpha/2),
                        quantile(vboot$IPA1,probs=1-alpha/2))),
               
               nrow=2,ncol=6, byrow=T, 
               dimnames = list(c('Brier','IPA'),
                                                       rep(c('Estimate','Lower .95 ','Upper .95'),2)))

res_ov_fgr<-round(res_ov_fgr,2) # Digits

res_ov<-round(res_ov_fgr,2) # Digits
kable(res_ov) %>% 
  kable_styling('striped', position ="center") %>%
  add_header_above(c(' '=1,
                     'Development data'=3, 
                     'Validation data'=3))

```

The overall performance measures seems better in the validation than in the development set.

### 2.2 Discrimination measures
Discrimination is the ability to differentiate between subjects who have the outcome and subjects who do not.
We propose to calculate:

+ Wolbers' C-index. More details are in the paper and in the references below;

+ Uno's time-dependent AUC measure of discrimination at a specific time horizon _t_ (i.e. five years in this case). More details are in the paper and in the references below;  

Values close to 1 indicate good discrimination ability, while values close to 0.5 indicated poor discrimination ability.  
We used the time horizon at 4.99 and not 5 years since controls are considered patients at risk after the time horizon.

```{r, discrimination,warning=FALSE,message=FALSE}

# C-index
# Time-dependent AUC

# Wolbers' C-index
# Apparent
C_rdata1<-unlist(pec::cindex(fit_fgr,cause=1,
                             eval.times = 4.99)$AppCindex)

# External validation
C_vdata1<-unlist(pec::cindex(fit_fgr,data=vdata,cause=1,
                             eval.times = 4.99)$AppCindex)

# Time dependent AUC
# Uno's time-dependent Area Under the Curve

# Apparent
Uno_rdata1<-
  timeROC(T=rdata$time, delta=rdata$status_num,
          marker=predict(fit_fg,newdata=rdata),
          cause=1,weighting='marginal',times=4.99,
          iid=TRUE)

# As an alternative use predict of FGR()
# Uno_rdata1<-
#  timeROC(T=rdata$time, delta=rdata$status_num,
#        marker=predict(fit_fgr,newdata=rdata,times=4.99),
#        cause=1,weighting='marginal',times=4.99,
#        iid=TRUE)

# External validation
Uno_vdata1<-
  timeROC(T=vdata$time, delta=vdata$status_num,
          marker=predict(fit_fg,newdata=vdata),
          cause=1,weighting='marginal',times=4.99,
          iid=TRUE)

# As an alternative use predict of FGR()
# Uno_vdata1<-
#  timeROC(T=vdata$time, delta=vdata$status_num,
#        marker=predict(fit_fgr,newdata=vdata,times=4.99),
#        cause=1,weighting='marginal',times=4.99,
#        iid=TRUE)

# NOTE: if you have a lot of data n > 2000, standard error computation may be really long.
# In that case, please use bootstrap percentile to calculate confidence intervals.
# NOTE: AUC_1: controls = subjects free of any event
# NOTE: AUC_2: controls = subjects not a case
# NOTE: AUC_2 was used.

# Bootstraping Wolbers' C-index to calculate the bootstrap percentile confidence intervals
C_boot1<-function(split) {unlist(pec::cindex(fit_fgr,
                                             data=analysis(split),
                                             cause=1,eval.times = 4.99)$AppCindex)}

# Run time-dependent AUC in the bootstrapped development and validation data
# to calculate the non-parametric CI through percentile bootstrap
rboot <- rboot %>% mutate(C1=map_dbl(splits,C_boot1))
vboot <- vboot %>% mutate(C1=map_dbl(splits,C_boot1))


alpha<-.05
k<-2
res_discr_fgr<-matrix(c(
  C_rdata1,
  quantile(rboot$C1,probs=alpha/2),
  quantile(rboot$C1,probs=1-alpha/2),
  
  C_vdata1,
  quantile(vboot$C1,probs=alpha/2),
  quantile(vboot$C1,probs=1-alpha/2),
  
  Uno_rdata1$AUC_2['t=4.99'],
  Uno_rdata1$AUC_2['t=4.99']-
    qnorm(1-alpha/2)*Uno_rdata1$inference$vect_sd_2['t=4.99'],
  Uno_rdata1$AUC_2['t=4.99']+
    qnorm(1-alpha/2)*Uno_rdata1$inference$vect_sd_2['t=4.99'],
  
  Uno_vdata1$AUC_2['t=4.99'],
  Uno_vdata1$AUC_2['t=4.99']-
    qnorm(1-alpha/2)*Uno_vdata1$inference$vect_sd_2['t=4.99'],
  Uno_vdata1$AUC_2['t=4.99']+
    qnorm(1-alpha/2)*Uno_vdata1$inference$vect_sd_2['t=4.99']
),

nrow=2,ncol=6, byrow=T, 
dimnames = list(c('Wolbers C','Uno AUC'),
                rep(c('Estimate','Lower .95 ','Upper .95'),2)))

res_discr_fgr<-round(res_discr_fgr,k)

kable(res_discr_fgr) %>% 
  kable_styling('striped', position ="center") %>%
  add_header_above(c(' '=1,'Development data'=3,
                     'Validation data'=3))
```

The time-dependent AUCs at 5 years were in the external validation were between 0.71 and 0.72.


### 2.3 Calibration
Calibration is the agreement between observed outcomes and predicted probabilities.
For example, in competing risks models, a predicted probability of the event of interest at a fixed time horizon _t_ of 80% is considered reliable if it can be expected that 80 out of 100 will experience the event of interest among patients received a predicted probability of 80%.

Calibration is measured by:

+ Observed and Expected ratio (OE ratio) at time horizon (*t*);  

+ Integrated Calibration Index (ICI): it is the weighted difference between smoothed observed proportions and predicted probabilities
in which observations are weighted by the empirical density function of the predicted probabilities;  

+ E50, E90 and Emax denote the median, the 90th percentile and the maximum absolute difference between observed and predicted probabilities of the outcome at time *t*;  

+ Calibration plot as a graphical representation of calibration.  
  
  

### 2.3.1 Observed/Expected ratio, ICI, E50, E90, Emax

We calculate the observed/ expected ratio (OE) at 5 years in the development and validation data.

```{r, OE, fig.align='center',warning=FALSE,message=FALSE}
# Load the function to calculate the OE ratio
source('C:\\Users\\danie\\Documents\\GitHub\\ValidationCompRisks\\Functions\\OE_function.R',local = knitr::knit_global())

# O = estimated cumulative incidence at 5 years
# E = mean of the predicted cumulative incidence at 5 years

obs_vdata<-1-summary(
  survfit(Surv(Tstart,Tstop,status==1)~1,
          data=vdata.w,weights=weight.cens),times=5)$surv
n_events_vdata<-sum(vdata$status_num==1)

# Observed/Expected ratio
OE_vdata<-OE_function(fit=fit_fgr,newdata=vdata,cause=1,
            thorizon=5,obs_cif=obs_vdata,
            obs_events=n_events_vdata)

# Calibration measures: ICI, E50, E90, Emax
source('C:\\Users\\danie\\Documents\\GitHub\\ValidationCompRisks\\Functions\\cal_measures.R',local = knitr::knit_global())

cal_measures(vdata,5,fit_fgr,Tstop='time',status='status_num',cause=1)

res_OE<-matrix(c(
  OE.rdata,
  OE.rdata1b,
  OE.vdata,
  OE.vdata1b
),
  nrow=1,ncol=12, byrow=T, 
  dimnames = list(c('O/E ratio'),
                rep(c('Estimate','Lower .95 ','Upper .95'),4)))

res_OE<-round(res_OE,2)
kable(res_OE) %>% 
  kable_styling('striped', position ='center') %>% 
    add_header_above(c(' '=1,'Apparent'=3, 'Apparent + PGR'=3,'External'=3,'External + PGR'=3))

```

The models PGR tend to slightly underpredict the risk of mortality in the validation data (OE: 1.11, 95%CI: 0.99-1.25 without PGR; OE: 1.08 95%CI: 0.96-1.21).  

### 2.3.2 Calibration plot using restricted cubic splines
Calibration plots of the external validation data with and without PGR are calculated and shown using restricted cubic splines.  
The interpretation of the calibration plot were provided in the section 2.3 of this document, in the corresponding paper and in the literature provided in the paper and at the end of this document. More details about the method are given in the references below.

+ on the _x-axis_ the predicted survival (or risk) probabilities at a fixed time horizon (e.g. at 5 years);

  + on the _y-axis_ the observed survival (or risk) probabilities at a fixed time horizon (e.g. at 5 years);

  + The 45-degree line indicates the good overall calibration. 
Points below the 45-degree line indicates that the model overestimate the observed risk. 
If points are above the 45-degree line, the model underestimate the observed risk;
The observed probabilities estimated by the Kaplan-Meier curves (in case of survival) or by the complementary of the Kaplan-Meier curves (in case of risk in absence of competing risks) are represented in terms of percentiles of the predicted survival (risk) probabilitie

```{r, cal_rcs, fig.align='center',warning=FALSE,message=FALSE}
## External data without PGR
evdata1$predmort5<-1-survest(s5,newdata=evdata1,times=5)$surv
evdata1$predmort5.cll<-log(-log(1-evdata1$predmort5))

# Estimate
vcalibrate.efit1<-cph(Surv(ryear,status)~rcs(predmort5.cll,3),x=T,y=T,
                      data=evdata1,surv=T)  # better rms::cph?
predict.grid<-seq(quantile(evdata1$predmort5,prob=0.01), quantile(evdata1$predmort5,prob=0.99),length=100)
predict.grid.cll<-log(-log(1-predict.grid))
predict.grid.df<-data.frame(predict.grid)
predict.grid.cll.df<-data.frame(predict.grid.cll)
names(predict.grid.df)<-'predmort5'
names(predict.grid.cll.df)<-'predmort5.cll'

# Plot
pred.vcalibrate.efit1<-1-survest(vcalibrate.efit1,newdata=predict.grid.cll.df,times=5)$surv
pred.vcalibrate.efit1.upper<-1-survest(vcalibrate.efit1,newdata=predict.grid.cll.df,times=5)$lower
pred.vcalibrate.efit1.lower<-1-survest(vcalibrate.efit1,newdata=predict.grid.cll.df,times=5)$upper
par(xaxs='i',yaxs='i',las=1)
plot(predict.grid,pred.vcalibrate.efit1,type='l',lty=1,xlim=c(0,1),
     ylim=c(0,1), lwd=2,
     xlab='Predicted probability',
     ylab='Observed probability', bty='n')
lines(predict.grid,pred.vcalibrate.efit1.lower,type='l',lty=2,lwd=2)
lines(predict.grid,pred.vcalibrate.efit1.upper,type='l',lty=2,lwd=2)
abline(0,1,lwd=2,lty=2,col='red')
title('A External data without PGR', adj=0)
#par(new=T)
#plot(density(evdata1$predmort5),axes=F,xlab=NA,ylab=NA,
#     main="")

## External data with PGR
evdata1$predmort5<-1-survest(s6,newdata=evdata1,times=5)$surv
evdata1$predmort5.cll<-log(-log(1-evdata1$predmort5))

# Estimate
vcalibrate.efit1b<-cph(Surv(ryear,status)~rcs(predmort5.cll,3),
                       x=T,y=T,data=evdata1,surv=T) 
predict.grid<-seq(quantile(evdata1$predmort5,prob=0.01), quantile(evdata1$predmort5,prob=0.99),length=100)
predict.grid.cll<-log(-log(1-predict.grid))
predict.grid.df<-data.frame(predict.grid)
predict.grid.cll.df<-data.frame(predict.grid.cll)
names(predict.grid.df)<-'predmort5'
names(predict.grid.cll.df)<-'predmort5.cll'

# Plot
pred.vcalibrate.efit1b<-1-survest(vcalibrate.efit1b,newdata=predict.grid.cll.df,times=5)$surv
pred.vcalibrate.efit1b.lower<-1-survest(vcalibrate.efit1b,newdata=predict.grid.cll.df,times=5)$upper
pred.vcalibrate.efit1b.upper<-1-survest(vcalibrate.efit1b,newdata=predict.grid.cll.df,times=5)$lower
par(xaxs='i',yaxs='i',las=1)
plot(predict.grid,pred.vcalibrate.efit1b,type='l',lty=1,xlim=c(0,1),
     ylim=c(0,1), lwd=2,
     xlab='Predicted probability of recurrence at 5 years',
     ylab='Observed probability of recurrence at 5 years', bty='n')
lines(predict.grid,pred.vcalibrate.efit1b.lower,type='l',lty=2,lwd=2)
lines(predict.grid,pred.vcalibrate.efit1b.upper,type='l',lty=2,lwd=2)
abline(0,1,lwd=2,lty=2,col='red')
title('B External data with PGR', adj=0)
#par(new=T)
#plot(density(evdata1$predmort5),axes=F,xlab=NA,ylab=NA,
#     main="")
```

Both plots identified good calibration although probabilities of recurrence were slightly underestimated especially for the lowest and the highest values of 
the observed probabilities of recurrence.  
The additional information of PGR improved the overall calibration, especially for the highest values, as shown in the two calibration plots above.


## Goal 3: Clinical utility
Discrimination and calibration measures are essential to assess the prediction performance but insufficient to evaluate the potential clinical utility of a risk prediction model for decision making. When new markers are available, clinical utility assessment evaluates whether the extended model helps to improve decision making.  
Clinical utility is measured by the net benefit that includes the number of true positives and the number of false positives. For example, in time-to-event models, the true positives reflect the benefit of being event free for a given time horizon using additional interventions such as additional treatments, personalized follow-up or additional surgeries. The false positives represent the harms of unnecessary interventions.   
Generally, in medicine, clinicians accepts to treat a certain number of patients for which interventions are unnecessary to be event free for a given time horizon. So, false negatives (the harm of not being event free for a given time horizon) are more important than false positives (the harm of unnecessary interventions). Thus, net benefit is the number of true positives classifications minus the false positives classifications weighted by a factor related to the harm of not preventing the event versus unnecessary interventions. The weighting is derived from the threshold probability to death (one minus survival probability) using a defined time horizon (for example 5 years since diagnosis). For example, a threshold of 10% implies that additional interventions for 10 patients of whom one would have experience the event in 5 years if untreated is acceptable (thus treating 9 unnecessary patients). This strategy is compared with the strategies of treat all and treat none patients. If overtreatment is harmful, a higher threshold should be used.  
The net benefit is calculated as:  
  
<img src="https://render.githubusercontent.com/render/math?math=%5Chuge%7B%5Cfrac%7BTP%7D%7Bn%7D-%5Cfrac%7BFP%7D%7Bn%7D*%5Cfrac%7Bp_t%7D%7B1-p_t%7D%7D">
  
*TP*=true positive patients   
*FP*=false positive patients  
*n*=number of patients and *p*<sub>t</sub> is the risk threshold.  

For survival data *TP* and *FP* is calculated as follows:   
<img src="https://render.githubusercontent.com/render/math?math=%5CLarge%7BTP%20%3D%20%5B1-S(t)%7C%20X%3D1%5D*P(X%3D1)*n%7D">
  
<img src="https://render.githubusercontent.com/render/math?math=%5CLarge%7BFP%20%3D%20%5BS(t)%7C%20X%3D1%5D*P(X%3D1)*n%7D">
  
  where  
*S(t)* survival at time *t*  
  *X=1* where the predicted probability at time *t* is  *p*<sub>t</sub>  
  
  And the the decision curve is calculated as follows:
  
1. Choose a time horizon (in this case 5 years);
2. Specify a risk threshold which reflects the ratio between harms and benefit of an additional intervention;
3. Calculate the number of true positive and false positive given the threshold specified in (2);
4. Calculate the net benefit of the survival model;
5. Plot net benefit on the *y-axis* against the risk threshold on the *x-axis*;
6. Repeat steps 2-4 for each model consideration;
7. Repeat steps 2-4 for the strategy of assuming all patients are treated;
8. Draw a straight line parallel to the *x-axis* at y=0 representing the net benefit associated with the strategy of assuming that all patients are not treated.

Given some thresholds, the model/strategy with higher net benefit represents the one that potentially improves  clinical decision making. However, poor discrimination and calibration lead to lower net benefit.

```{r, function_stdca, message=FALSE,warning=FALSE, fig.align='center',include=FALSE}
# Run the function to calculate the net benefit and the elements needed to develop decision curve analysis
source('C:\\Users\\danie\\Documents\\GitHub\\Prediction_performance_survival\\Functions\\stdca.R',local = knitr::knit_global())
```


```{r, dca, message=FALSE,warning=FALSE, fig.align='center'}
# We could use the rms::survest() or pec::predictEventProb() to get the mortality at observed time
# Development data
# Predicted probability calculation
edata1$mort5_efit1<-1-predictSurvProb(efit1,newdata=edata1,times=5)

# Extended model with PGR
# Predicted probability calculation
edata1$mort5_efit1b<-1-predictSurvProb(efit1b,newdata=edata1,times=5)

# Run decision curve analysis

# Development data
# Model without PGR
edata1<-as.data.frame(edata1)
dca_rdata_1<-stdca(data=edata1,outcome="status",ttoutcome = "ryear",
                    timepoint=5,predictors="mort5_efit1",xstop=1.0,
                    ymin=-0.01, graph=FALSE)
# Model with PGR
dca_rdata_1b<-stdca(data=edata1,outcome='status',ttoutcome = "ryear",
                     timepoint=5,predictors='mort5_efit1b',xstop=1.0,
                     ymin=-0.01, graph=FALSE)

# Decision curves plot
par(xaxs='i',yaxs='i',las=1)
plot(dca_rdata_1$net.benefit$threshold,
     dca_rdata_1$net.benefit$mort5_efit1,type='l',lwd=2,lty=1,
     xlab='Threshold probability in %', ylab='Net Benefit',
     xlim=c(0,1),ylim=c(-0.10,0.45),bty='n',
     cex.lab=1.2,cex.axis=1)
# legend('topright',c('Treat all','Treat none','Prediction model'),
#        lwd=c(2,2,2),lty=c(1,1,2),col=c('darkgray','black','black'),bty='n')
lines(dca_rdata_1$net.benefit$threshold,dca_rdata_1$net.benefit$none,type='l',lwd=2, lty=4)
lines(dca_rdata_1$net.benefit$threshold,dca_rdata_1$net.benefit$all,type='l',lwd=2,col='darkgray')
lines(dca_rdata_1b$net.benefit$threshold,dca_rdata_1b$net.benefit$mort5_efit1b,type='l',lwd=2,lty=5)
legend('topright',
       c('Treat All',
         'Original model',
         'Original model + PGR',
         'Treat None'), lty=c(1,1,5,4),lwd=2,col=c('darkgray','black','black','black'),
       bty='n')
title("A Development data",adj=0,cex=1.5)


# External data
# Validation data
# Predicted probability calculation
evdata1$mort5_model1<-1-predictSurvProb(efit1,newdata=evdata1,times=5)

# Extended model with PGR
# Predicted probability calculation
evdata1$mort5_model1b<-1-predictSurvProb(efit1b,newdata=evdata1,times=5)

# Run decision curve analysis

# Development data
# Model without PGR
evdata1<-as.data.frame(evdata1)
dca_vdata_model1<-stdca(data=evdata1,outcome='status',ttoutcome = "ryear",
                         timepoint=5,predictors='mort5_model1',xstop=1.0,
                         ymin=-0.01, graph=FALSE)
# Model with PGR
dca_vdata_model1b<-stdca(data=evdata1,outcome='status',ttoutcome = "ryear",
                          timepoint=5,predictors='mort5_model1b',xstop=1,
                          ymin=-0.01, graph=FALSE)

# Decision curves plot
par(xaxs='i',yaxs='i',las=1)
plot(dca_vdata_model1$net.benefit$threshold,
     dca_vdata_model1$net.benefit$mort5_model1,type='l',lwd=2,lty=1,
     xlab='Threshold probability in %', ylab='Net Benefit',
     xlim=c(0,1),ylim=c(-0.10,0.60),bty='n',
     cex.lab=1.2,cex.axis=1)
# legend('topright',c('Treat all','Treat none','Prediction model'),
#        lwd=c(2,2,2),lty=c(1,1,2),col=c('darkgray','black','black'),bty='n')
lines(dca_vdata_model1$net.benefit$threshold,dca_vdata_model1$net.benefit$none,type='l',lwd=2, lty=4)
lines(dca_vdata_model1$net.benefit$threshold,dca_vdata_model1$net.benefit$all,type='l',lwd=2,col='darkgray')
lines(dca_vdata_model1b$net.benefit$threshold,dca_vdata_model1b$net.benefit$mort5_model1b,type='l',lwd=2,lty=5)
legend('topright',
       c('Treat All',
         'Original model',
         'Original model + PGR',
         'Treat None'), lty=c(1,1,5,4),lwd=2,col=c("darkgray","black","black","black"),
       bty='n')
title("B External data",adj=0,cex=1.5)
```

Based on previous research we used a range of thresholds from 14% to 23% for adjuvant chemotherapy. If we choose a threshold of 20% the model had a net benefit of 0.262 in the development data using the basic model. This means that the model would identify 26 patients per 100 who will have recurrent breast cancer or die within 5 years since diagnosis and thus adjuvant chemotherapy is really needed.   
The decision curve shows that the net benefit would be much larger for higher threshold values, i.e., patients accepting higher risks of recurrence compared to the treat all strategy. The same interpretation may be used in the validation data:
  choosing a threshold of 20% the basic model had a net benefit of 0.385 for the basic and the extended model.  
Moreover, net benefit can be defined in terms of reduction of avoidable interventions (e.g adjuvant chemotherapy per 100 patients) by:
  
<img src="https://render.githubusercontent.com/render/math?math=%5Chuge%7B%5Cfrac%7BNB_%7Bmodel%7D%20-%20NB_%7Ball%7D%7D%7B(p_t%2F%20(1-p_t))%7D*100%7D%0A">  
  
  where *NB*<sub>model</sub> is the net benefit of the prediction model, *NB*<sub>all</sub> is the net benefit of the strategy treat all and $p_{t}$ is the risk threshold.


## References
+ Overall measures   
Reference: https://diagnprognres.biomedcentral.com/articles/10.1186/s41512-018-0029-2*/ \
R Vignette: https://cran.r-project.org/web/packages/riskRegression/vignettes/IPA.html#fn.1 \

+ Discrimination measures   
References: \
https://www.jstor.org/stable/27639883 \
https://onlinelibrary.wiley.com/doi/10.1002/sim.5958 \

+ Calibration \
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3933449/pdf/nihms542648.pdf \
https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8281   \
https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8570   \

+ Clinical utility (decision curves)  
R/SAS/STATA code and references: 
  https://www.mskcc.org/departments/epidemiology-biostatistics/biostatistics/decision-curve-analysis \
More guidelines about net benefit assessment and interpretation \
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6261531/ \
https://diagnprognres.biomedcentral.com/articles/10.1186/s41512-019-0064-7 \

+ Other useful references \
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6728752/ \
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7100774/ \



